# OZNAL - Project 01 - Classification and Regression Hypothesis

## FIIT STU - Bratislava

## March 2024

## Authors

-   Leonard Puškáč
-   Ema Richnáková

## Dataset

In this mini-project we will analyse data from <https://www.kaggle.com/datasets/julianoorlandi/spotify-top-songs-and-audio-features?resource=download>

The dataset contains information about various "top" songs and their stats on Spotify. There are 6513 entries each containing FILL IN features, such as the name of the song, its id, features that can be used to classify the songs such as the key or the mode (Major/Minor), as well as numeric features that can be used for regression analysis - energy, danceability, speechiness (the amount of spoken word), liveness, loudness, tempo and many others.

### Data Description

| Column name      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|------------------------------------|------------------------------------|
| id               | The Spotify ID for the track.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| artist_names     | The name of the artist.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| track_name       | The name of the track.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| source           | The name of the record label.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| key              | The key the track is in.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| mode             | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived.                                                                                                                                                                                                                                                                                                                                                                                              |
| time_signature   | An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7.                                                                                                                                                                                                                                                                                                                                  |
| danceability     | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.                                                                                                                                                                                                                                                                       |
| energy           | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.                                                                                                                          |
| speechiness      | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
| acousticness     | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.                                                                                                                                                                                                                                                                                                                                                                                       |
| instrumentalness | Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.                                                                                                                 |
| liveness         | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.                                                                                                                                                                                                                                                                                            |
| valence          | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).                                                                                                                                                                                                                                                                  |
| loudness         | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.                                                                                                                                                                                     |
| tempo            | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.                                                                                                                                                                                                                                                                                                                         |
| duration_ms      | The duration of the track in milliseconds.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| weeks_on_chart   | Number of weeks the track was in the top 200 charts.                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| streams          | How many streams the track had during its period in the charts.                                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Importing Libraries

```{r message=FALSE}
library(magrittr)
library(tidyverse)
library(dplyr) # mutate
library(caret) # confusion matrix
library(nortest) # Anderson-Darling test for normality
library(ROCit) # rocit() - optimal cut-off
library(pROC) # roc() - create ROC curve
library(MASS)
```

## Loading the Dataset

```{r}
data <- read_csv("spotify_top_songs_audio_features.csv", col_names = TRUE, num_threads = 4)
head(data)
```

## Data Exploration (for regression models... mostly)

First we select the numeric features of the data set, so that we can perform correlation analysis.

```{r}
numeric_columns <- names(data)[sapply(data, is.numeric)]
numeric_columns
numeric_data <- data %>% select(numeric_columns)
```

```{r}
numeric_correlation_matrix <- cor(numeric_data)
heatmap(numeric_correlation_matrix, 
        col = colorRampPalette(c("blue", "white", "red"))(100),
        symm = TRUE)
numeric_correlation_matrix
```

## Interpreting the Correlation Matrix and Forming Hyptheses for the Regression Models

We have selected the numeric features of the data set to see which pairs of features are worth exploring in terms of forming a hypothesis when it comes to regression. From the heatmap (and the printed numeric values) representing the correlation of these sets of numbers, we can see that there are pairs of features that clearly correlate a lot more (positive correlation relationship) than others - for example the pairs:

-   loudness - energy
-   streams - weeks_on_chart

These two pairs of features have the highest correlation coefficients among all the pairs. For our purposes we will probably be disregarding the streams an weeks_on_chart pair as forming a hypothesis about this pair is not as interesting (it seems rather clear that with more time on the chart the number of streams would increase - although it would be interesting to explore where we can expect a drop-off or what the out-liers of this trend are). We will therefore be focusing on the loudness and energy pair.

As well as that, we can see that there are certain pairs of features that have a really high negative correlation relationship (a correlation coefficient close to -1, rather than 1), meaning that as the value of one variable increases the other one tends to decreases. These pairs are:

-   acousticness - loudness
-   acousticness - energy

A high number close to either of the extremes (-1 and 1) means that there is a strong linear relationship between the variables (negative or positive) - it is therefore "safe" to make assumptions about these pairs.

However, there are still some pairs of features that might be worth exploring further in spite of their unimpressive correlation coefficients. These are features that don't correlate as much as the previously mentioned pairs, but ones which correlate (positively or negatively) strongly enough to be worth a consideration:

-   acousticness - danceability
-   valence - energy
-   valence - danceability

### Forming the Hypotheses

We have identified the pairs of variables which have a high enough correlation relationship for them to be interesting to us.

acousticness strongly negatively correlates with loudness and energy, and somewhat strongly with danceability Furthermore, those three variables do not correlate much with each other (apart from the energy and loudness pair - but thats okay, we can experiment with it). This makes them ideal candidates for being the predictor variables for acousticness acousticness \~ danceability + energy + loudness

The relationship between energy and loudness can also be explored further. We can see that energy also somewhat correlates with valence, and that valence and loudness do not correlate as much with each other. So we will be using this set of features for the second hypothesis. energy \~ loudness + valence

#### Hypothesis 1

H1: Acousticness tends to decrease with danceability.

H0: There is no relationship between acousticness and danceability

#### Hypothesis 2

H1: Acousticness tends to decrease with loudness.

H0: There is no relationship between acousticness and loudness

#### Hypothesis 3

H1: Energy tends to increase with valence.

H0: There is no relationship between energy and valence

## Regression models

First lets split the data set into training and testing subsets.

```{r}
sample <- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob = c(0.8, 0.2))
train_data <- data[sample, ]
test_data <- data[!sample, ]
```

Then we can start thinking about which models to use to test our hypothesis. For that we need to find out the distributions of our predicted variables.

### 1st Model

```{r}
# Find out the distribution of the acousticness variable
hist(data$acousticness, col="green")
```

We can see that the feature is not normally distributed, but that it resembles a Gamma distribution - negatively skewed. We can therefore use the general linear model, using the Gamma family. We also have to specify that link="log" for it to work with a negatively skewed distribution.

```{r}
model_h1 <- train_data %>% glm(formula = acousticness ~ danceability + loudness + energy, family=Gamma(link="log"))
summary(model_h1)
```

From the model summary we can see that there are two statistically significant pieces of information. And that is - the acousticness of the song tends to decrease with danceability (by about 0.71 per increase of 1) and it tends to decrease in energy by about 2,39 per increase of one.

For Hypothesis 1 we can safely reject the null hypothesis, as we can clearly see that the results are statistically significant with a p value \< 0.05.

For the second hypothesis (acousticness tends to decrease with loudness) we cannot come to the same conclusion - the p value is about 0.3 - much larger than 0.05, so we cannot safely assume that there is a statistical relationship between the two features.

The following is a plot of the model residuals...

We can see that the residuals vs. fitted graph does sort of form a funnel shape directed to the left, instead of the right. This suggests heteroscedasticity. Meaning that we might have inflated t-statistics for smaller fitted values. We might be able to remedy this by applying some transformations to the predicted variable - such as Box-Cox

```{r}
par(mfrow = c(1,3)) # par() sets up a plotting grid: 1 row x 3 columns
plot(model_h1, which = c(1,2,5))
```

# 

### 2nd Model

```{r}
hist(data$energy, col="green")
```

Here we can see a visually different kind of distribution - but it belongs to the same family as with the previous model - it is not a normal distribution, so we will be using GLM. and it is positively skewed and it resembles a Gamma distribution.

```{r}
model_h2 <- train_data %>% glm(formula = energy ~ loudness + valence, family=Gamma(link="log"))
summary(model_h2)
```

From the model summary we can discern a few things - first the intercept is -0.03 which tells us about the predicted value of the energy when both loudness and valence are at zero - again not a realistic scenario. Second is that both predictors have a statistically significant relationship to the predicted variable - p values are less than 0.05.

We can therefore safel reject the null hypothesis for Hypothesis 3 - energy tends to increase with valence. We can see that it tends to increase by about 0.19 with every step.

We can also see that valance has a higher impact then loudness on the energy of the song.

The following is a graph of the residuals summary from the model. We can see that the residuals are concentrated around a central value. The quantiles while not perfect look good. and that there are no residuals that would have too high of a leverage score.

```{r}
par(mfrow = c(1,3)) 
plot(model_h2, which = c(1,2,5))
```

## Classification models

We are considering 3 columns, that we could classify as 1 and 0. And those are:

-   Key

    -   [1] A-C#/Db
    -   [0] D-G#/Ab

    ```{r}
    table(data$key)
    ```

-   Mode

    -   [1] Major
    -   [0] Minor

    ```{r}
    table(data$mode)
    ```

-   Time Signature

    -   [1] 4 beats
    -   [0] *other*

    ```{r}
    table(data$time_signature)
    ```

For Hypothesis 1 we choose *mode* and for Hypothesis 2 we choose *time signature*.

Then on every hypothesis, we should also check, if investigated data in hypothesis are normally distributed or not. This is crutial information before choosing right classification model for our hypothesis.

What are the differences between classification models?

-   Logistic Regression model
    -   assumes that relationship between predictors and the log odds of outcome is linear (but also can handle nonlinear relationships)
    -   may be sensitive to outliers
-   Linear Discriminant Analysis (LDA) model
    -   assumes that predicots are normally distributed in each class, making it robust to outliers
    -   finds linear combination of predictors, that best separates classes

### Hypothesis 1

*Songs classified as major mode (1) are more likely to have higher danceability and energy scores compared to songs classified as minor mode (0).*

#### Data exploration

For this hypothesis, we wanted balanced classification, so we chose *mode*.

Classification - *mode*:

-   [1] Major
-   [0] Minor

```{r}
class_data <- data %>%
  mutate(mode = ifelse(mode == "Major", 1, 0)) %>%
  select_if(is.numeric)
head(class_data)
```

We contstructed Hypothesis 1 based on information from colerration heatmap.

```{r}
heatmap(cor(class_data),
        col = colorRampPalette(c("darkgreen", "white", "red"))(100),
        symm = TRUE)
```

#### Check data normal distribution

First, we check, if data are normally distributed. We choose Anderson-Darling test for normality, because there are present tied values in data (it means that multiple rows have same value).

```{r}
ad.test(class_data$danceability + class_data$energy)$p.value
```

P-value \< $\alpha = 0.05$. That means we reject $H_0$ of this test, which states, that data are normally distributed.

We can also check it visually with histogram. On histogram we can also see, that data are not normally distributed.

```{r}
hist(class_data$danceability + class_data$energy, col = "green")
```

According to this observations, we choose **Logistic Regression** model.

#### Sampling

Before creating glm model, we split data to train and test sample. It will helps as predict and test, how is model well trained.

```{r}
sample <- sample(c(TRUE, FALSE), nrow(class_data), replace = TRUE, prob = c(0.7, 0.3))
train_data <- class_data[sample, ]
test_data <- class_data[!sample, ]
```

#### Model

Now we can train classification model.

```{r}
model_glm <- glm(mode ~ danceability + energy, data = train_data, family = binomial)
summary(model_glm)
plot(model_glm, which = 1)
```

A Pearson residual close to zero indicates that observed response is close to predicted probability, suggesting a good fit between model and data.

```{r}
coef(model_glm)
```

**Negative coefficients** suggest, that lower *danceability* and *energy* are associated with higher likelihood of being *major mode*.

That coeficcient suggestion can be visually represented as in this plot.

```{r}
plot(class_data$danceability + class_data$energy, class_data$mode, xlab = "Danceability + Energy", ylab = "Mode", pch = 19, col = ifelse(class_data$mode == 1, "green", "red"))
abline(model_glm, col = "black", lwd = 2)
legend("topright", legend = c("Major Mode", "Minor Mode", "Logistic Regression Line"), col = c("green", "red", "black"), pch = c(1, 1, NA), lwd = c(NA, NA, 2))
```

**Result of Hypothesis 1:** Hypothesis is rejected. Higher danceability and energy scores are associated with *minor mode*.

Anyway, we have model, so we can predict model is trained with test data. But before that, we must choose cut-off (threshold of classification to major/minor). We can get optimal cut-off with help of **ROC curves**.

```{r}
roc <- rocit(class = model_glm$y, score = model_glm$fitted.values)
par(mfrow = c(1, 2))
plot(roc)
ksplot(roc)
par(mfrow = c(1, 1))
summary(roc)
```

Now we can get prediction classes with optimal cut-off and draw ROC curve.

```{r}
predicted_classes <- predict(model_glm, newdata = test_data, type = "response")
predicted_classes <- ifelse(predicted_classes > roc$AUC, 1, 0)

roc_curve <- roc(test_data$mode, predicted_classes)
plot(roc_curve, col = "green", main = "ROC Curve")
```

We can evaluate our model with **confusion matrix** and other metrics mentioned below.

```{r}
confusion_matrix <- caret::confusionMatrix(as_factor(predicted_classes), as_factor(test_data$mode), positive = "1")
ct <- confusion_matrix$table

TP <- ct[2, 2] # true positive
FP <- ct[2, 1] # false positive
TN <- ct[1, 1] # true negative
FN <- ct[1, 2] # false negative
P <- TP + FN # all positives (1)
N <- FP + TN # all negatives (0)

# confusion matrix heatmap
ggplot(as.data.frame(ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

**Accuracy** indicates overall model performance. Higher accuracy suggests better predictive performance, but it may not provide complete picture, especially in imbalanced datasets.

```{r}
print(paste("Accuracy: ", (TP + TN) / (P + N)))
```

But in our case, dataset is balanced, because *major* and *minor mode* data are divided nearly 50/50. So our model is performing slightly better than random predictions.

```{r}
print(paste("Proportion of major mode (1): ", P / (P + N)))
print(paste("Proportion of minor mode (0): ", N / (P + N)))
```

**Precision** measures ability of model to avoid FP. Higher precision indicates fewer FP, meaning fewer instances are incorrectly predicted as *major mode*.

```{r}
precision <- TP / (FP + TP)
print(paste("Precision: ", precision))
```

**Recall** measures ability of model to capture all actual positives. Higher recall indicates that model captures higher proportion of actual *major mode* instances.

```{r}
recall <- TP / P
print(paste("Recall: ", recall))
```

**F1-score** provides balanced measure between precision and recall. It is useful when there is an uneven class distribution.

```{r}
f1_score <- 2 * (precision * recall) / (precision + recall)
print(paste("F1 score: ", f1_score))
```

**Specificity** measures how well model performs in correct identified instances of the negative class. High specificity indicates that model has low FP rate, meaning it effectively identifies TN without misclassifying them as positives.

```{r}
print(paste("Specificity: ", TN / (TN + FP)))
```

### Hypothesis 2

*Songs with 4 beats (1) tend be less accoustic and have lower instrumentalness score.*

#### Data exploration

For this hypothesis, we wanted unbalanced classification, so we chose *time_signature*.

Classification - *time_signature*:

-   [1] 4 beats
-   [0] *other*

```{r}
class_data <- data %>%
  mutate(time_signature = ifelse(time_signature == "4 beats", 1, 0)) %>%
  select_if(is.numeric)
head(class_data)
```

We contstructed Hypothesis 2 based on information from colerration heatmap.

```{r}
heatmap(cor(class_data),
        col = colorRampPalette(c("darkgreen", "white", "red"))(100),
        symm = TRUE)
```

#### Check data normal distribution

First, we check, if data are normally distributed. We choose Anderson-Darling test for normality.

```{r}
ad.test(class_data$acousticness + class_data$instrumentalness)$p.value
```

P-value \< $\alpha = 0.05$. That means we reject $H_0$ of this test, which states, that data are normally distributed.

We can also check it visually with histogram. On histogram we can also see, that data are not normally distributed.

```{r}
hist(class_data$acousticness + class_data$instrumentalness, col = "green")
```

According to this observations, we choose **Logistic Regression** model.

#### Sampling

Before creating glm model, we split data to train and test sample. It will helps as predict and test, how is model well trained.

```{r}
sample <- sample(c(TRUE, FALSE), nrow(class_data), replace = TRUE, prob = c(0.7, 0.3))
train_data <- class_data[sample, ]
test_data <- class_data[!sample, ]
```

#### Model

Now we can train classification model.

```{r}
model_glm <- glm(time_signature ~ acousticness + instrumentalness, data = train_data, family = binomial)
summary(model_glm)
plot(model_glm, which = 1)
```

A Pearson residual close to zero indicates that observed response is close to predicted probability, suggesting a good fit between model and data.

```{r}
coef(model_glm)
```

**Negative coefficients** suggest, that lower *acousticness* and *instrumentalness* are associated with *4 beats time signature*.

That coeficcient suggestion can be visually represented as in this plot.

```{r}
plot(class_data$acousticness + class_data$instrumentalness, class_data$time_signature, xlab = "acousticness + instrumentalness", ylab = "time_signature", pch = 19, col = ifelse(class_data$time_signature == 1, "green", "red"))
abline(model_glm, col = "black", lwd = 2)
legend("topright", legend = c("4 beats", "other", "Logistic Regression Line"), col = c("green", "red", "black"), pch = c(1, 1, NA), lwd = c(NA, NA, 2))
```

**Result of Hypothesis 2:** Hypothesis is accepted. Lower acousticness and instrumentalness scores are associated with *4 beats time signature*.

Anyway, we have model, so we can predict model is trained with test data. But before that, we must choose cut-off (threshold of classification to 4 beats/other). We can try to get optimal cut-off with help of **ROC curves**.

```{r}
roc <- rocit(class = model_glm$y, score = model_glm$fitted.values)
par(mfrow = c(1, 2))
plot(roc)
ksplot(roc)
par(mfrow = c(1, 1))
summary(roc)
```

After some testing, we figured out, that optimal cut-off from `roc$AUC` is not optimal. We tried some different values and cut-off = 0.9 was probably most optimal for our unbalanced classification.

Now we can get prediction classes with defined cut-off and draw ROC curve.

```{r}
predicted_classes <- predict(model_glm, newdata = test_data, type = "response")
predicted_classes <- ifelse(predicted_classes > 0.9, 1, 0)

roc_curve <- roc(test_data$time_signature, predicted_classes)
plot(roc_curve, col = "green", main = "ROC Curve")
```

We can evaluate our model with **confusion matrix** and other metrics mentioned below.

```{r}
confusion_matrix <- caret::confusionMatrix(as_factor(predicted_classes), as_factor(test_data$time_signature), positive = "1")
ct <- confusion_matrix$table

TP <- ct[2, 2] # true positive
FP <- ct[2, 1] # false positive
TN <- ct[1, 1] # true negative
FN <- ct[1, 2] # false negative
P <- TP + FN # all positives (1)
N <- FP + TN # all negatives (0)

# confusion matrix heatmap
ggplot(as.data.frame(ct), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green") +
  labs(x = "Actual", y = "Predicted", title = "Confusion Matrix Heatmap")
```

**Accuracy** indicates overall model performance. Higher accuracy suggests better predictive performance, but it may not provide complete picture, especially in imbalanced datasets.

```{r}
print(paste("Accuracy: ", (TP + TN) / (P + N)))
```

Our dataset is very unbalanced, because *4 beats* and *other* time signatures are divided nearly 90/10. So we can't rely on this metrics.

```{r}
print(paste("Proportion of major mode (1): ", P / (P + N)))
print(paste("Proportion of minor mode (0): ", N / (P + N)))
```

**Precision** measures ability of model to avoid FP. Higher precision indicates fewer FP, meaning fewer instances are incorrectly predicted as *4 beats*.

```{r}
precision <- TP / (FP + TP)
print(paste("Precision: ", precision))
```

**Recall** measures ability of model to capture all actual positives. Higher recall indicates that model captures higher proportion of actual *4 beats* instances.

```{r}
recall <- TP / P
print(paste("Recall: ", recall))
```

**F1-score** provides balanced measure between precision and recall. It is useful when there is an uneven class distribution.

```{r}
f1_score <- 2 * (precision * recall) / (precision + recall)
print(paste("F1 score: ", f1_score))
```

**Specificity** measures how well model performs in correct identified instances of the negative class. High specificity indicates that model has low FP rate, meaning it effectively identifies TN without missclassifying them as positives.

```{r}
print(paste("Specificity: ", TN / (TN + FP)))
```
